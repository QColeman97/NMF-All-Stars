{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "Non-negative matrix factorization (NMF) is an unsupervised machine learning technique created by [Lee & Seung](http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf) in 1999. It is a versatile algorithm because of its ability to make a parts-based-representation of its input data. What enables this parts-based-representation is the constraint that NMF's input data must be quantitative with no negative values. \n",
    "\n",
    "### **How it Works**\n",
    "Given a non-negative matrix ***V*** of dimension *f* ✕ *t*, the algorithm learns two non-negative matrices: ***W*** of dimension *f* ✕ *k* and ***H*** of dimension *k* ✕ *t*, where k < minimum(f,t). ***W*** and ***H*** are approximate factors of ***V***, thus when they are multiplied together, they create an approximation of the original matrix called ***V'***.\n",
    "\n",
    "- ***V*** is the original data\n",
    "    - t columns of f-dimensional data\n",
    "    - Each column is a sample, each row is a feature\n",
    "- ***W*** is the basis vectors (or dictionary matrix)\n",
    "    - A linear combination of these approximates any sample in V\n",
    "    - Each column is called a basis vector\n",
    "- ***H*** is the activations\n",
    "    - Each activation encodes a linear combination of all basis vectors, and corresponds to a sample in V\n",
    "    - Each column is called an activation (or weight or gain)\n",
    "\n",
    "To put it simply, basis vectors are like the building blocks to create any sample in our input data, and an activation tells us how much of each building block to use to recreate a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF](NMF.png)\n",
    "\n",
    "figure by Qwertyus - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=29114677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF has many uses. It can naturally be used for data compression and approximation - if k is small enough, like k = 2 in the figure above, ***W*** and ***H*** take up less space than the original data ***V***. It also has data-specific uses like source-seperation for audio data or topic extraction for textual data. A use of NMF we'll explore is **dimensionality reduction**. \n",
    "\n",
    "### **Dimensionality Reduction**\n",
    "Dimensionality reduction is the task of taking a dataset with many dimesions (or features), and transforming it into a dataset with fewer dimensions while losing the least amount of information possible.\n",
    "\n",
    "We already learned a dimensionality reduction technique in class: Principal Components Analysis (PCA). The principal components (PCs) in PCA describe the axes orthogonal to each other that run in the direction of greatest variance in the data. Thus, these principal components can describe the data in lower dimension.\n",
    "\n",
    "![PCA](PCA.png)\n",
    "\n",
    "Figure by https://medium.com/@TheDataGyan/dimensionality-reduction-with-pca-and-t-sne-in-r-2715683819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the principal components of PCA, the basis vectors of NMF accomplish the same thing. If you think about it, basis vectors in the linear algebra sense are unit vectors that describe a vector space. To \"describe a vector space\" means any possible vector can be made with a linear combination of these basis vectors. Principal components are like specifically-designed basis vectors for losing the least amount of information possible. For example, in the figure above, if a dataset is being dimension-reduced by only 1 dimension, information will be lost but only on the axis of least variance (PC1 and PC2 are the axes of greatest variance).\n",
    "\n",
    "So in NMF, as long we choose a k-value that is less than the number of dimensions in our dataset, we'll create k basis vectors, and reduce our dataset down to k dimensions. Since each datapoint in a dimension-reduced dataset is a linear combination of basis vectors, our dimension-reduced dataset is simply the matrix ***H***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation**\n",
    "In order to make NMF, we frame an optimization problem. This optimization problem is about minimizing the distance/error from the product ***WH*** to ***V***. The implementation of NMF we are using is derived from a specific distance measurement called Kullback-Leibler (KL) Divergence. KL Divergence mathematically allows us to create two multiplicative update formulas, one for each of the matrices: ***W*** and ***H***.\n",
    "\n",
    "In this algorithm, ***W*** and ***H*** are initialized to random-valued matrices in an unsupervised manner, and ***V*** is input. Then, for a predetermined number of iterations (usually 100-200 until convergence), the multiplicative updates are applied to ***W*** and ***H*** in succession until their matrix product doesn't approximate the original data any much better with continuing iterations. The figure below sums up this algorithm as \"KL-NMF\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF learn loop](NMF_Learn_Loop.png)\n",
    "\n",
    "Figure by https://ccrma.stanford.edu/~njb/teaching/sstutorial/part2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dimensionality reduction, one more step to take is to normalize the basis vector matrix after initialization and each update. This normalization insures that the basis vectors remain unit vectors, as the only thing that matters is their orientation. With that said, let's code this below. We'll put this algorithm inside the primary method of our NMF class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
