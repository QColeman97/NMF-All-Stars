{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Non-negative Matrix Factorization - An Implementation and Experimentation**\n",
    "### CSC 466-1 Knowledge Discovery from Data, Winter 2020 - Final Project\n",
    "#### Andrew Kesheshian, Griffin Johnson, Quinn Coleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "Non-negative matrix factorization (NMF) is an unsupervised machine learning technique created by [Lee & Seung](http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf) in 1999. It is a versatile algorithm because of its ability to make a parts-based-representation of its input data. What enables this parts-based-representation is the constraint that NMF's input data must be quantitative with no negative values. \n",
    "\n",
    "#### **How it Works**\n",
    "Given a non-negative matrix ***V*** of dimension *f* ✕ *t*, the algorithm learns two non-negative matrices: ***W*** of dimension *f* ✕ *k* and ***H*** of dimension *k* ✕ *t*, where k < minimum(f,t). ***W*** and ***H*** are approximate factors of ***V***, thus when they are multiplied together, they create an approximation of the original matrix called ***V'***.\n",
    "\n",
    "- ***V*** is the original data\n",
    "    - t columns of f-dimensional data\n",
    "    - Each column is a sample, each row is a feature\n",
    "- ***W*** is the basis vectors (or dictionary matrix)\n",
    "    - A linear combination of these approximates any sample in V\n",
    "    - Each column is called a basis vector\n",
    "- ***H*** is the activations\n",
    "    - Each activation encodes a linear combination of all basis vectors, and corresponds to a sample in V\n",
    "    - Each column is called an activation (or weight or gain)\n",
    "\n",
    "To put it simply, basis vectors are like the building blocks to create any sample in our input data, and an activation tells us how much of each building block to use to recreate a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF](NMF.png)\n",
    "\n",
    "Figure by Qwertyus - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=29114677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF has many uses. It can naturally be used for data compression and approximation - if k is small enough, like k = 2 in the figure above, ***W*** and ***H*** take up less space than the original data ***V***. It also has data-specific uses like source-seperation for audio data or topic extraction for textual data. A use of NMF we'll explore is **dimensionality reduction**. \n",
    "\n",
    "#### **Dimensionality Reduction**\n",
    "Dimensionality reduction is the task of taking a dataset with many dimesions (or features), and transforming it into a dataset with fewer dimensions while losing the least amount of information possible.\n",
    "\n",
    "We already learned a dimensionality reduction technique in class: Principal Components Analysis (PCA). The principal components (PCs) in PCA describe the axes orthogonal to each other that run in the direction of greatest variance in the data. Thus, these principal components can describe the data in lower dimension.\n",
    "\n",
    "![PCA](PCA.png)\n",
    "\n",
    "Figure by https://medium.com/@TheDataGyan/dimensionality-reduction-with-pca-and-t-sne-in-r-2715683819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the principal components of PCA, the basis vectors of NMF accomplish the same thing. If you think about it, basis vectors in the linear algebra sense are unit vectors that describe a vector space. To \"describe a vector space\" means any possible vector can be made with a linear combination of these basis vectors. Principal components are like specifically-designed basis vectors for losing the least amount of information possible. For example, in the figure above, if a dataset is being dimension-reduced by only 1 dimension, information will be lost but only on the axis of least variance (PC1 and PC2 are the axes of greatest variance).\n",
    "\n",
    "So in NMF, as long we choose a k-value that is less than the number of dimensions in our dataset, we'll create k basis vectors, and reduce our dataset down to k dimensions. Since each datapoint in a dimension-reduced dataset is a linear combination of basis vectors, our dimension-reduced dataset is simply the matrix ***H***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation**\n",
    "In order to make NMF, we frame an optimization problem. This optimization problem is about minimizing the distance/error from the product ***WH*** to ***V***. The implementation of NMF we are using is derived from a specific distance measurement called Kullback-Leibler (KL) Divergence. KL Divergence mathematically allows us to create two multiplicative update formulas, one for each of the matrices: ***W*** and ***H***.\n",
    "\n",
    "In this algorithm, ***W*** and ***H*** are initialized to random-valued matrices in an unsupervised manner, and ***V*** is input. Then, for a predetermined number of iterations (usually 100-200 until convergence), the multiplicative updates are applied to ***W*** and ***H*** in succession until their matrix product doesn't approximate the original data any much better with continuing iterations. The figure below sums up this algorithm as \"KL-NMF\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF learn loop](NMF_Learn_Loop.png)\n",
    "\n",
    "Figure by https://ccrma.stanford.edu/~njb/teaching/sstutorial/part2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dimensionality reduction, one more step to take is to normalize the basis vector matrix after initialization and each update. This normalization insures that the basis vectors remain unit vectors, as the only thing that matters is their orientation. With that said, let's code this below. We'll put this algorithm inside the primary method of our NMF class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# If I don't normalize the basis vectors, transformed values upper limit ~20 on average by sight\n",
    "# If I do, upper limit ~15 on average by sight\n",
    "\n",
    "class OurNMF:\n",
    "    def __init__(self, n_components, learn_iter=200):\n",
    "        self.n_components_ = n_components\n",
    "        self.learn_iter_ = learn_iter\n",
    "        \n",
    "    # Private method - core functionality\n",
    "    def __nmf(self, input_df):\n",
    "        input_matrix = input_df.to_numpy().T\n",
    "        activations = np.random.rand(self.n_components_, input_matrix.shape[1])\n",
    "        basis_vectors = np.random.rand(input_matrix.shape[0], self.n_components_)\n",
    "\n",
    "#         print('Before normalization')\n",
    "#         print(basis_vectors)\n",
    "        basis_vectors = normalize(basis_vectors)\n",
    "#         print('After normalization')\n",
    "#         print(basis_vectors)\n",
    "\n",
    "        ones = np.ones(input_matrix.shape)\n",
    "    \n",
    "        for i in range(self.learn_iter_):\n",
    "            activations *= ((basis_vectors.T @ (input_matrix / (basis_vectors @ activations))) / (basis_vectors.T @ ones))\n",
    "            basis_vectors *= (((input_matrix / (basis_vectors @ activations)) @ activations.T) / (ones @ activations.T))\n",
    "            \n",
    "#             if i == 0:\n",
    "#                 print('Before normalization')\n",
    "#                 print(basis_vectors)\n",
    "            basis_vectors = normalize(basis_vectors)\n",
    "#             if i == 0:\n",
    "#                 print('After normalization')\n",
    "#                 print(basis_vectors)\n",
    "            \n",
    "        return basis_vectors.T, activations.T\n",
    "    \n",
    "    # Public methods\n",
    "    # Useless function - similar to scikit-learn, but call fit_transform to get transformed values back\n",
    "    def fit(self, input_df):\n",
    "        self.basis_vecs_, self.trans_vals_ = self.__nmf(input_df)\n",
    "        \n",
    "    def fit_transform(self, input_df):\n",
    "        self.basis_vecs_, self.trans_vals_ = self.__nmf(input_df)\n",
    "        return self.trans_vals_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Validate our Implementation**\n",
    "Let's validate our implementation of NMF by comparing its results with the NMF method provided by a trusted and widely-used machine learning library.\n",
    "\n",
    "This library is called [scikit-learn](https://scikit-learn.org/stable/) and its NMF documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html).\n",
    "\n",
    "***Note -- Scikit-Learn Documentation Error:***\n",
    "On the NMF documentation page, variable names *W* and *H* are swapped which can be very **misleading**. They call *W* the transformed data, and *H* the factorization matrix (dictionary elements). In reality, *W* is this factorization matrix, and *H* is the transformed data *(no documentation is perfect, even for scikit-learn)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Creation**\n",
    "To begin, let's make a dataset for our NMFs. We use another python library, [pandas](https://pandas.pydata.org/), to handle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>43.76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>43.45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>54.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>40.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>89.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.43</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>49.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>34.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  C   D      E  K  L  M  ans\n",
       "0  1.00  3   9  43.76  1  0  0    1\n",
       "1  0.99  3   8  43.45  0  1  0    0\n",
       "2  0.54  0  23  54.65  0  0  1    1\n",
       "3  1.00  0  12  40.15  0  0  1    0\n",
       "4  0.32  1  12  89.04  0  1  0    1\n",
       "5  0.43  4  11  49.04  0  1  0    0\n",
       "6  0.99  1  10  34.54  0  0  1    0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dummy_data = [{'A': 1.00, 'B': 'K', 'C': 3, 'D': 9, 'E': 43.76, 'ans': 1},\n",
    "              {'A': 0.99, 'B': 'L', 'C': 3, 'D': 8, 'E': 43.45, 'ans': 0},\n",
    "              {'A': 0.54, 'B': 'M', 'C': 0, 'D': 23, 'E': 54.65, 'ans': 1},\n",
    "              {'A': 1.00, 'B': 'M', 'C': 0, 'D': 12, 'E': 40.15, 'ans': 0},\n",
    "              {'A': 0.32, 'B': 'L', 'C': 1, 'D': 12, 'E': 89.04, 'ans': 1},\n",
    "              {'A': 0.43, 'B': 'L', 'C': 4, 'D': 11, 'E': 49.04, 'ans': 0},\n",
    "              {'A': 0.99, 'B': 'M', 'C': 1, 'D': 10, 'E': 34.54, 'ans': 0}]\n",
    "\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "# Transform categorical features ('B') into quantitative ones ('K', 'L', 'M') via get_dummies (one-hot encoding)\n",
    "quant = pd.get_dummies(dummy_df['B'])\n",
    "labels = dummy_df['ans']\n",
    "dummy_df = dummy_df.drop('B', axis=1).drop('ans', axis=1)\n",
    "dummy_df = pd.concat((dummy_df, quant, labels), axis=1)\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comparison**\n",
    "A good way to see if our NMF works is by performing a common use case for dimensionality reduction: visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/decomposition/nmf.py:1035: ConvergenceWarning: Maximum number of iteration 200 reached. Increase it to improve convergence.\n",
      "  \" improve convergence.\" % max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2b43b740f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "sklearn_nmf = NMF(n_components=2, init='random', solver='mu', beta_loss='kullback-leibler')\n",
    "our_nmf = OurNMF(n_components=2)\n",
    "\n",
    "sklearn_trans_df = pd.DataFrame(sklearn_nmf.fit_transform(dummy_df), columns=['BV1', 'BV2'])\n",
    "our_trans_df = pd.DataFrame(our_nmf.fit_transform(dummy_df), columns=['BV1', 'BV2'])\n",
    "\n",
    "# display(sklearn_trans_df)\n",
    "# display(our_trans_df)\n",
    "\n",
    "sklearn_trans_df.plot.scatter(x='BV1', y='BV2', title='Scikit-Learn\\'s NMF Dim-Reduced Dataset')\n",
    "our_trans_df.plot.scatter(x='BV1', y='BV2', title='Our NMF Dim-Reduced Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great! The data on both charts look similar, which means that both our and scikit-learn's NMF find the same way to represent this data in lower dimension. 👏\n",
    "\n",
    "*Note -- if you don't see a similarity between plots and the axes look flipped:* Run the previous code cell 1-2 more times until they match. This happens because in NMF each basis vector equally contributes to recreating the dataset, so their ordering isn't meaningful, thus not needing consistency. We just want to see when the two basis vectors from our NMF is in the same order as scikit-learn NMF's two basis vectors - when this happens, the transformed values corresponding to a basis vector will appear on the same axis as the other chart's axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "#### To-fill-in next:\n",
    "- Experimentation = Comparisons of our NMF to PCA\n",
    "    - 1) Compare prediction performance between a weak classifer given NMF-dim-reduced data & a weak classifier given PCA-dim-reduced data (synthetic dataset)\n",
    "        - analysis: observe the differences in performance, and maybe see if NMF is better or worse than PCA for this use-case\n",
    "    - 2) Make a Pearson correlation matrix between NMF and PCA on BVs/PCs and/or transformed values\n",
    "        - analysis: see if NMF and PCA is correlated (hopefully not - casue theyre different methods)\n",
    "    - ? 3) Compare visualization of data when we do NMF and PCA on titanic dataset to bring it down to 2 dimensions for charting (scatter plots)\n",
    "        - analysis: see if NMF or PCA visualizes the titanic data better, or maybe if one's better at visualizing previously-quantitative (i.e. pclass) / quantitative (i.e. age) features than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use Case: Too Many Dimensions for a Weak Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        float64\n",
       "1        float64\n",
       "2        float64\n",
       "3        float64\n",
       "4        float64\n",
       "          ...   \n",
       "996      float64\n",
       "997      float64\n",
       "998      float64\n",
       "999      float64\n",
       "class      int64\n",
       "Length: 1001, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "n_features = 1000\n",
    "signed_synth_data, labels = make_classification(n_samples=100, n_features=n_features, n_informative=3, n_classes=3)\n",
    "\n",
    "# To Make our data non-negative, we'll scale our data with min-max scaling which puts it into the range of [0, 1]\n",
    "scaler = MinMaxScaler()\n",
    "synth_data = scaler.fit_transform(signed_synth_data)\n",
    "\n",
    "y = pd.DataFrame(labels, columns=[\"class\"]).astype(int)\n",
    "X = pd.DataFrame(synth_data)\n",
    "Xy = pd.concat([X, y], axis=1)\n",
    "\n",
    "Xy.head()\n",
    "Xy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Avg Accuracy Score (Bad): 0.3853333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Problem: simple classifiers like linear regression handle poorly with datasets that have:\n",
    "# - massive dimension (feature count) compared to sample size\n",
    "# - few important features\n",
    "\n",
    "pred_iterations = 30\n",
    "accs = []\n",
    "logreg_model = LogisticRegression()\n",
    "for _ in range(pred_iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    y_pred = logreg_model.predict(X_test)\n",
    "    \n",
    "#     print('Y Preds')\n",
    "#     print(len(y_pred))\n",
    "#     print('Y Test')\n",
    "#     print(len(y_test))\n",
    "    \n",
    "    acc_score = accuracy_score(y_pred, y_test)\n",
    "    accs.append(acc_score)\n",
    "\n",
    "print('Logistic Regression Avg Accuracy Score (Bad):', np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of continuous and multiclass targets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e6e10d59b8a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mlinreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinreg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mnmf_accs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pcatrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mdiffering_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         raise ValueError(\"Classification metrics can't handle a mix of {0} \"\n\u001b[0;32m---> 81\u001b[0;31m                          \"and {1} targets\".format(type_true, type_pred))\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;31m# We can't have more than one value on y_type => The set is no more needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Classification metrics can't handle a mix of continuous and multiclass targets"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Solution: simple classifiers can handle these datasets better, after dimensionality reduction is applied\n",
    "# If we reduce dimensions on a dataset via NMF & sklearn PCA, and then predict at each dimension drop, does lin. reg. performance increase?\n",
    "min_dim = 2\n",
    "\n",
    "dim_nmf_accs, dim_pca_accs = [], []\n",
    "# Track how prediction performance differs with amount of dimensions\n",
    "\n",
    "# for dim_num in range(100, n_features + 1):\n",
    "for dim_num in range(min_dim, min(X.shape) + 1): # num of dim must be <= min dimension of our X for sklearn NMF\n",
    "\n",
    "#     # debug print\n",
    "#     print('Orig df (dim_num = ' + str(dim_num) + '):')\n",
    "#     display(X)\n",
    "    \n",
    "    # Perform PCA and NMF on same dataset\n",
    "    nmf_model = OurNMF(n_components=dim_num)\n",
    "    X_nmftrans = pd.DataFrame(nmf_model.fit_transform(X))\n",
    "\n",
    "#     # debug print\n",
    "#     print('Dim-red df (dim_num = ' + str(dim_num) + '):')\n",
    "#     display(X_nmftrans)\n",
    "    \n",
    "    pca_model = PCA(n_components=dim_num)\n",
    "    X_pcatrans = pd.DataFrame(pca_model.fit_transform(X))\n",
    "    \n",
    "    nmf_accs, pca_accs = [], []\n",
    "    for _ in range(pred_iterations):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_nmftrans, y)\n",
    "        logreg_model.fit(X_train, y_train)\n",
    "        y_pred = logreg_model.predict(X_test)\n",
    "        nmf_accs.append(accuracy_score(y_pred, y_test))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pcatrans, y)\n",
    "        logreg_model.fit(X_train, y_train)\n",
    "        y_pred = logreg_model.predict(X_test)\n",
    "        pca_accs.append(accuracy_score(y_pred, y_test))\n",
    "\n",
    "    dim_nmf_accs.append(np.mean(nmf_accs))\n",
    "    dim_pca_accs.append(np.mean(pca_accs))\n",
    "\n",
    "pd.Series(dim_nmf_accs).plot.line()\n",
    "pd.Series(dim_pca_accs).plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
