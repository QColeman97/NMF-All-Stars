{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Non-negative Matrix Factorization - An Implementation and Experimentation**\n",
    "### CSC 466-1 Knowledge Discovery from Data, Winter 2020 - Final Project\n",
    "#### Andrew Kesheshian, Griffin Johnson, Quinn Coleman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "Non-negative matrix factorization (NMF) is an unsupervised machine learning technique created by [Lee & Seung](http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf) in 1999. It is a versatile algorithm because of its ability to make a parts-based-representation of its input data. What enables this parts-based-representation is the constraint that NMF's input data must be quantitative with no negative values. \n",
    "\n",
    "### **How it Works**\n",
    "Given a non-negative matrix ***V*** of dimension *f* âœ• *t*, the algorithm learns two non-negative matrices: ***W*** of dimension *f* âœ• *k* and ***H*** of dimension *k* âœ• *t*, where k < minimum(f,t). ***W*** and ***H*** are approximate factors of ***V***, thus when they are multiplied together, they create an approximation of the original matrix called ***V'***.\n",
    "\n",
    "- ***V*** is the original data\n",
    "    - t columns of f-dimensional data\n",
    "    - Each column is a sample, each row is a feature\n",
    "- ***W*** is the basis vectors (or dictionary matrix)\n",
    "    - A linear combination of these approximates any sample in V\n",
    "    - Each column is called a basis vector\n",
    "- ***H*** is the activations\n",
    "    - Each activation encodes a linear combination of all basis vectors, and corresponds to a sample in V\n",
    "    - Each column is called an activation (or weight or gain)\n",
    "\n",
    "To put it simply, basis vectors are like the building blocks to create any sample in our input data, and an activation tells us how much of each building block to use to recreate a sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF](NMF.png)\n",
    "\n",
    "Figure by Qwertyus - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=29114677"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF has many uses. It can naturally be used for data compression and approximation - if k is small enough, like k = 2 in the figure above, ***W*** and ***H*** take up less space than the original data ***V***. It also has data-specific uses like source-seperation for audio data or topic extraction for textual data. A use of NMF we'll explore is **dimensionality reduction**. \n",
    "\n",
    "### **Dimensionality Reduction**\n",
    "Dimensionality reduction is the task of taking a dataset with many dimesions (or features), and transforming it into a dataset with fewer dimensions while losing the least amount of information possible.\n",
    "\n",
    "We already learned a dimensionality reduction technique in class: Principal Components Analysis (PCA). The principal components (PCs) in PCA describe the axes orthogonal to each other that run in the direction of greatest variance in the data. Thus, these principal components can describe the data in lower dimension.\n",
    "\n",
    "![PCA](PCA.png)\n",
    "\n",
    "Figure by https://medium.com/@TheDataGyan/dimensionality-reduction-with-pca-and-t-sne-in-r-2715683819"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the principal components of PCA, the basis vectors of NMF accomplish the same thing. If you think about it, basis vectors in the linear algebra sense are unit vectors that describe a vector space. To \"describe a vector space\" means any possible vector can be made with a linear combination of these basis vectors. Principal components are like specifically-designed basis vectors for losing the least amount of information possible. For example, in the figure above, if a dataset is being dimension-reduced by only 1 dimension, information will be lost but only on the axis of least variance (PC1 and PC2 are the axes of greatest variance).\n",
    "\n",
    "So in NMF, as long we choose a k-value that is less than the number of dimensions in our dataset, we'll create k basis vectors, and reduce our dataset down to k dimensions. Since each datapoint in a dimension-reduced dataset is a linear combination of basis vectors, our dimension-reduced dataset is simply the matrix ***H***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Implementation**\n",
    "In order to make NMF, we frame an optimization problem. This optimization problem is about minimizing the distance/error from the product ***WH*** to ***V***. The implementation of NMF we are using is derived from a specific distance measurement called Kullback-Leibler (KL) Divergence. KL Divergence mathematically allows us to create two multiplicative update formulas, one for each of the matrices: ***W*** and ***H***.\n",
    "\n",
    "In this algorithm, ***W*** and ***H*** are initialized to random-valued matrices in an unsupervised manner, and ***V*** is input. Then, for a predetermined number of iterations (usually 100-200 until convergence), the multiplicative updates are applied to ***W*** and ***H*** in succession until their matrix product doesn't approximate the original data any much better with continuing iterations. The figure below sums up this algorithm as \"KL-NMF\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NMF learn loop](NMF_Learn_Loop.png)\n",
    "\n",
    "Figure by https://ccrma.stanford.edu/~njb/teaching/sstutorial/part2.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For dimensionality reduction, one more step to take is to normalize the basis vector matrix after initialization and each update. This normalization insures that the basis vectors remain unit vectors, as the only thing that matters is their orientation. With that said, let's code this below. We'll put this algorithm inside the primary method of our NMF class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "class OurNMF:\n",
    "    def __init__(self, n_components, learn_iter=200):\n",
    "        self.n_components_ = n_components\n",
    "        self.learn_iter_ = learn_iter\n",
    "        \n",
    "    # Private method - core functionality\n",
    "    def __nmf(self, input_df):\n",
    "        input_matrix = input_df.to_numpy().T\n",
    "        activations = np.random.rand(self.n_components_, input_matrix.shape[1])\n",
    "        basis_vectors = np.random.rand(input_matrix.shape[0], self.n_components_)\n",
    "        basis_vectors = normalize(basis_vectors)\n",
    "        ones = np.ones(input_matrix.shape)\n",
    "    \n",
    "        for i in range(self.learn_iter_):\n",
    "            activations *= ((basis_vectors.T @ (input_matrix / (basis_vectors @ activations))) / (basis_vectors.T @ ones))\n",
    "            basis_vectors *= (((input_matrix / (basis_vectors @ activations)) @ activations.T) / (ones @ activations.T))\n",
    "            \n",
    "            activations = np.nan_to_num(activations)\n",
    "            basis_vectors = np.nan_to_num(basis_vectors)\n",
    "#             print(basis_vectors.dtype)\n",
    "#             print(True in np.isnan(basis_vectors))\n",
    "#             print(True in np.isinf(basis_vectors))\n",
    "            \n",
    "            basis_vectors = normalize(basis_vectors)\n",
    "            \n",
    "        return basis_vectors.T, activations.T\n",
    "    \n",
    "    # Public methods\n",
    "    # Useless function - similar to scikit-learn, but call fit_transform to get transformed values back\n",
    "    def fit(self, input_df):\n",
    "        self.basis_vecs_, self.trans_vals_ = self.__nmf(input_df)\n",
    "        \n",
    "    def fit_transform(self, input_df):\n",
    "        self.basis_vecs_, self.trans_vals_ = self.__nmf(input_df)\n",
    "        return self.trans_vals_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Validate our Implementation**\n",
    "Let's validate our implementation of NMF by comparing its results with the NMF method provided by a trusted and widely-used machine learning library.\n",
    "\n",
    "This library is called [scikit-learn](https://scikit-learn.org/stable/) and its NMF documentation can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html).\n",
    "\n",
    "***Note -- Scikit-Learn Documentation Error:***\n",
    "On the NMF documentation page, variable names *W* and *H* are swapped which can be very **misleading**. They call *W* the transformed data, and *H* the factorization matrix (dictionary elements). In reality, *W* is this factorization matrix, and *H* is the transformed data *(no documentation is perfect, even for scikit-learn)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Dataset Creation**\n",
    "To begin, let's make a dataset for our NMFs. We use another python library, [pandas](https://pandas.pydata.org/), to handle datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>K</th>\n",
       "      <th>L</th>\n",
       "      <th>M</th>\n",
       "      <th>ans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>43.76</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.99</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>43.45</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>54.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>40.15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.32</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>89.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.43</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>49.04</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.99</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>34.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      A  C   D      E  K  L  M  ans\n",
       "0  1.00  3   9  43.76  1  0  0    1\n",
       "1  0.99  3   8  43.45  0  1  0    0\n",
       "2  0.54  0  23  54.65  0  0  1    1\n",
       "3  1.00  0  12  40.15  0  0  1    0\n",
       "4  0.32  1  12  89.04  0  1  0    1\n",
       "5  0.43  4  11  49.04  0  1  0    0\n",
       "6  0.99  1  10  34.54  0  0  1    0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dummy_data = [{'A': 1.00, 'B': 'K', 'C': 3, 'D': 9, 'E': 43.76, 'ans': 1},\n",
    "              {'A': 0.99, 'B': 'L', 'C': 3, 'D': 8, 'E': 43.45, 'ans': 0},\n",
    "              {'A': 0.54, 'B': 'M', 'C': 0, 'D': 23, 'E': 54.65, 'ans': 1},\n",
    "              {'A': 1.00, 'B': 'M', 'C': 0, 'D': 12, 'E': 40.15, 'ans': 0},\n",
    "              {'A': 0.32, 'B': 'L', 'C': 1, 'D': 12, 'E': 89.04, 'ans': 1},\n",
    "              {'A': 0.43, 'B': 'L', 'C': 4, 'D': 11, 'E': 49.04, 'ans': 0},\n",
    "              {'A': 0.99, 'B': 'M', 'C': 1, 'D': 10, 'E': 34.54, 'ans': 0}]\n",
    "\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "# Transform categorical features ('B') into quantitative ones ('K', 'L', 'M') via get_dummies (one-hot encoding)\n",
    "quant = pd.get_dummies(dummy_df['B'])\n",
    "labels = dummy_df['ans']\n",
    "dummy_df = dummy_df.drop('B', axis=1).drop('ans', axis=1)\n",
    "dummy_df = pd.concat((dummy_df, quant, labels), axis=1)\n",
    "dummy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Comparison**\n",
    "A good way to see if our NMF works is by performing a common use case for dimensionality reduction: visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f6cad112550>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "sklearn_nmf = NMF(n_components=2, init='random', solver='mu', beta_loss='kullback-leibler')\n",
    "our_nmf = OurNMF(n_components=2)\n",
    "\n",
    "sklearn_trans_df = pd.DataFrame(sklearn_nmf.fit_transform(dummy_df), columns=['BV1', 'BV2'])\n",
    "our_trans_df = pd.DataFrame(our_nmf.fit_transform(dummy_df), columns=['BV1', 'BV2'])\n",
    "\n",
    "# display(sklearn_trans_df)\n",
    "# display(our_trans_df)\n",
    "\n",
    "sklearn_trans_df.plot.scatter(x='BV1', y='BV2', title='Scikit-Learn\\'s NMF Dim-Reduced Dataset')\n",
    "our_trans_df.plot.scatter(x='BV1', y='BV2', title='Our NMF Dim-Reduced Dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great, our NMF works! ðŸŽ‰ðŸŽ‰ Since the data on both charts look similar, it seems that both our and scikit-learn's NMF find the same way to represent this data in lower dimension.\n",
    "\n",
    "*Note -- if you don't see a similarity between plots and the axes look flipped:* Run the previous code cell 1-2 more times until they match. This happens because in NMF each basis vector equally contributes to recreating the dataset, so their ordering isn't meaningful, thus not needing consistency. We just want to see when the two basis vectors from our NMF is in the same order as scikit-learn NMF's two basis vectors - when this happens, the transformed values corresponding to a basis vector will appear on the same axis as the other chart's axis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Experimentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "#### To-fill-in next:\n",
    "- Experimentation = Comparisons of our NMF to PCA\n",
    "    - 1) Compare prediction performance between a weak classifer given NMF-dim-reduced data & a weak classifier given PCA-dim-reduced data (synthetic dataset)\n",
    "        - analysis: observe the differences in performance, and maybe see if NMF is better or worse than PCA for this use-case\n",
    "    - 2) Make a Pearson correlation matrix between NMF and PCA on BVs/PCs and/or transformed values\n",
    "        - analysis: see if NMF and PCA is correlated (hopefully not - casue theyre different methods)\n",
    "    - ? 3) Compare visualization of data when we do NMF and PCA on titanic dataset to bring it down to 2 dimensions for charting (scatter plots)\n",
    "        - analysis: see if NMF or PCA visualizes the titanic data better, or maybe if one's better at visualizing previously-quantitative (i.e. pclass) / quantitative (i.e. age) features than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Use Case: Resolving the Curse of Dimensionality**\n",
    "Too Many Dimensions for a Weak Classifier. Let's begin by pre-processing a real dataset into quantitative, non-negative form. We'll be sure to fill in missing values, and scale our data so that no features with large values unfairly skew the learning of our classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1309, 720)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>pclass 1</th>\n",
       "      <th>pclass 2</th>\n",
       "      <th>pclass 3</th>\n",
       "      <th>female</th>\n",
       "      <th>male</th>\n",
       "      <th>1</th>\n",
       "      <th>...</th>\n",
       "      <th>Winnipeg, MB</th>\n",
       "      <th>Winnipeg, MN</th>\n",
       "      <th>Woodford County, KY</th>\n",
       "      <th>Worcester, England</th>\n",
       "      <th>Worcester, MA</th>\n",
       "      <th>Yoevil, England / Cottage Grove, OR</th>\n",
       "      <th>Youngstown, OH</th>\n",
       "      <th>Zurich, Switzerland</th>\n",
       "      <th>home.dest unknown</th>\n",
       "      <th>survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.361169</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412503</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.009395</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.295806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.022964</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.295806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.373695</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.295806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.311064</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.295806</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 720 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  sibsp     parch      fare  pclass 1  pclass 2  pclass 3  female  \\\n",
       "0  0.361169  0.000  0.000000  0.412503         1         0         0       1   \n",
       "1  0.009395  0.125  0.222222  0.295806         1         0         0       0   \n",
       "2  0.022964  0.125  0.222222  0.295806         1         0         0       1   \n",
       "3  0.373695  0.125  0.222222  0.295806         1         0         0       0   \n",
       "4  0.311064  0.125  0.222222  0.295806         1         0         0       1   \n",
       "\n",
       "   male  1  ...  Winnipeg, MB  Winnipeg, MN  Woodford County, KY  \\\n",
       "0     0  0  ...             0             0                    0   \n",
       "1     1  0  ...             0             0                    0   \n",
       "2     0  0  ...             0             0                    0   \n",
       "3     1  0  ...             0             0                    0   \n",
       "4     0  0  ...             0             0                    0   \n",
       "\n",
       "   Worcester, England  Worcester, MA  Yoevil, England / Cottage Grove, OR  \\\n",
       "0                   0              0                                    0   \n",
       "1                   0              0                                    0   \n",
       "2                   0              0                                    0   \n",
       "3                   0              0                                    0   \n",
       "4                   0              0                                    0   \n",
       "\n",
       "   Youngstown, OH  Zurich, Switzerland  home.dest unknown  survived  \n",
       "0               0                    0                  0         1  \n",
       "1               0                    0                  0         1  \n",
       "2               0                    0                  0         0  \n",
       "3               0                    0                  0         0  \n",
       "4               0                    0                  0         0  \n",
       "\n",
       "[5 rows x 720 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "titanic_df = pd.read_csv(\n",
    "    \"https://raw.githubusercontent.com/dlsun/data-science-book/master/data/titanic.csv\"\n",
    ")\n",
    "\n",
    "titanic_df = titanic_df.drop(\"name\", axis=1)\n",
    "titanic_df = titanic_df.drop(\"ticket\", axis=1)\n",
    "\n",
    "# 0) filling missing data\n",
    "titanic_df = titanic_df.replace([np.inf, -np.inf], np.nan)\n",
    "titanic_df[\"age\"] = titanic_df[\"age\"].fillna(value=titanic_df[\"age\"].mean())\n",
    "titanic_df[\"fare\"] = titanic_df[\"fare\"].fillna(value=0.0)\n",
    "\n",
    "# 1) Scale the quantitative vars\n",
    "quant_feat = [\"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "scaler = MinMaxScaler()\n",
    "quant_titanic_df = titanic_df[quant_feat]\n",
    "quant_titanic_df = pd.DataFrame(scaler.fit_transform(quant_titanic_df), columns=quant_feat)\n",
    "\n",
    "titanic_df = titanic_df.drop(quant_feat, axis=1)\n",
    "titanic_df = pd.concat((titanic_df, quant_titanic_df), axis=1)\n",
    "\n",
    "# 2) Encode catagorical variables to quantitative variables\n",
    "catagorical_to_quantitative_maps = {}\n",
    "\n",
    "# pclass\n",
    "pclass_quant = pd.get_dummies(titanic_df[\"pclass\"])\n",
    "pclass_quant[\"pclass 1\"] = pclass_quant[1]\n",
    "pclass_quant = pclass_quant.drop(1, axis=1)\n",
    "pclass_quant[\"pclass 2\"] = pclass_quant[2]\n",
    "pclass_quant = pclass_quant.drop(2, axis=1)\n",
    "pclass_quant[\"pclass 3\"] = pclass_quant[3]\n",
    "pclass_quant = pclass_quant.drop(3, axis=1)\n",
    "titanic_df = titanic_df.drop('pclass', axis=1)\n",
    "titanic_df = pd.concat((titanic_df, pclass_quant), axis=1)\n",
    "\n",
    "# gender\n",
    "catagorical_to_quantitative_maps[\"sex\"] = list(titanic_df[\"sex\"].unique())\n",
    "gender_catagories = pd.DataFrame(pd.get_dummies(titanic_df[\"sex\"]))\n",
    "titanic_df = titanic_df.drop(\"sex\", axis = 1)\n",
    "titanic_df= pd.concat([titanic_df, gender_catagories], axis = 1)\n",
    "   \n",
    "# boat\n",
    "titanic_df[\"boat\"] = titanic_df[\"boat\"].fillna(value=\"No boat\")\n",
    "catagorical_to_quantitative_maps[\"boat\"] = list(titanic_df[\"boat\"].unique())\n",
    "boat_catagories = pd.DataFrame(pd.get_dummies(titanic_df[\"boat\"]))\n",
    "titanic_df = titanic_df.drop(\"boat\", axis = 1)\n",
    "titanic_df = pd.concat([titanic_df, boat_catagories], axis = 1)\n",
    "# adjust a few columns to aviod running into problems when creating a column with the same name\n",
    "# will need to consider this when going backwards\n",
    "titanic_df[\"boat C\"] = titanic_df[\"C\"]\n",
    "titanic_df = titanic_df.drop(\"C\", axis=1)\n",
    "catagorical_to_quantitative_maps[\"boat\"].append(\"boat C\")\n",
    "catagorical_to_quantitative_maps[\"boat\"].remove(\"C\")\n",
    "titanic_df[\"boat D\"] = titanic_df[\"D\"]\n",
    "titanic_df = titanic_df.drop(\"D\", axis=1)\n",
    "catagorical_to_quantitative_maps[\"boat\"].append(\"boat D\")\n",
    "catagorical_to_quantitative_maps[\"boat\"].remove(\"D\")\n",
    "\n",
    "#cabin\n",
    "catagorical_to_quantitative_maps[\"cabin\"] = list(titanic_df[\"cabin\"].unique())\n",
    "cabins = pd.DataFrame(pd.get_dummies(titanic_df[\"cabin\"]))\n",
    "titanic_df = titanic_df.drop(\"cabin\", axis = 1)\n",
    "titanic_df = pd.concat([titanic_df, cabins], axis = 1)\n",
    "\n",
    "#embarked\n",
    "titanic_df[\"embarked\"] = titanic_df[\"embarked\"].fillna(value=\"entrance unknown\")\n",
    "catagorical_to_quantitative_maps[\"embarked\"] = list(titanic_df[\"embarked\"].unique())\n",
    "entrances = pd.DataFrame(pd.get_dummies(titanic_df[\"embarked\"]))\n",
    "titanic_df = titanic_df.drop(\"embarked\", axis = 1)\n",
    "titanic_df = pd.concat([titanic_df, entrances], axis = 1)\n",
    "\n",
    "#body\n",
    "titanic_df[\"body\"] = titanic_df[\"body\"].fillna(value=\"body unknown\")\n",
    "catagorical_to_quantitative_maps[\"body\"] = list(titanic_df[\"body\"].unique())\n",
    "bodies = pd.DataFrame(pd.get_dummies(titanic_df[\"body\"]))\n",
    "titanic_df = titanic_df.drop(\"body\", axis = 1)\n",
    "titanic_df = pd.concat([titanic_df, bodies], axis = 1)\n",
    "\n",
    "#home.dest\n",
    "titanic_df[\"home.dest\"] = titanic_df[\"home.dest\"].fillna(value=\"home.dest unknown\")\n",
    "catagorical_to_quantitative_maps[\"home.dest\"] = list(titanic_df[\"home.dest\"].unique())\n",
    "destinations = pd.DataFrame(pd.get_dummies(titanic_df[\"home.dest\"]))\n",
    "titanic_df = titanic_df.drop(\"home.dest\", axis = 1)\n",
    "titanic_df = pd.concat([titanic_df, destinations], axis = 1)\n",
    "\n",
    "# Put class labels on the end\n",
    "survived = titanic_df['survived']\n",
    "titanic_df = titanic_df.drop('survived', axis=1)\n",
    "titanic_df = pd.concat((titanic_df, survived), axis=1)\n",
    "\n",
    "display(titanic_df.shape)\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do ourselves a NMF non-negativity constraint sanity check, and see if we have any negative values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of neg vals: 0\n"
     ]
    }
   ],
   "source": [
    "print('Number of neg vals:', (titanic_df < 0).sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope! So, now let's make this dataset hard for a classifier by using a small amount of samples, and adding many unimportant features - these are essentially noise for a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2720)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_samples = 200\n",
    "Xy = titanic_df.sample(frac=1.0)[:n_samples]\n",
    "Xy.index = range(n_samples)\n",
    "\n",
    "# 3) Put random vars in (already in min-max scale range)\n",
    "noise = pd.DataFrame(np.random.random(size=(n_samples, 2000)), columns=['noise_f' + str(x) for x in range(2000)])\n",
    "Xy = pd.concat((noise, Xy), axis=1)\n",
    "\n",
    "X = Xy.drop('survived', axis=1)\n",
    "y = Xy['survived']\n",
    "\n",
    "display(Xy.shape)\n",
    "Xy.head()\n",
    "\n",
    "X.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Avg Accuracy Score (Bad): 0.8253333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Problem: simple classifiers like linear regression handle poorly with datasets that have:\n",
    "# - massive dimension (feature count) compared to sample size\n",
    "# - few important features\n",
    "\n",
    "pred_iterations = 30\n",
    "accs = []\n",
    "logreg_model = LogisticRegression()\n",
    "for _ in range(pred_iterations):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    logreg_model.fit(X_train, y_train)\n",
    "    y_pred = logreg_model.predict(X_test)\n",
    "    \n",
    "    acc_score = accuracy_score(y_pred, y_test)\n",
    "    accs.append(acc_score)\n",
    "\n",
    "print('Logistic Regression Avg Accuracy Score (Bad):', np.mean(accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: divide by zero encountered in true_divide\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in matmul\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "# Solution: simple classifiers can handle these datasets better, after dimensionality reduction is applied\n",
    "# If we reduce dimensions on a dataset via NMF & sklearn PCA, and then predict at each dimension drop, does lin. reg. performance increase?\n",
    "min_dim = 2\n",
    "dim_nmf_accs, dim_pca_accs = [], []\n",
    "\n",
    "# Track how prediction performance differs with amount of dimensions\n",
    "# num of dim must be <= min dimension of our X for sklearn NMF\n",
    "for dim_num in range(min_dim, min(X.shape) + 1, 10): \n",
    "\n",
    "    # Perform PCA and NMF on same dataset\n",
    "    nmf_model = OurNMF(n_components=dim_num)\n",
    "    X_nmftrans = pd.DataFrame(nmf_model.fit_transform(X))\n",
    "    \n",
    "    pca_model = PCA(n_components=dim_num)\n",
    "    X_pcatrans = pd.DataFrame(pca_model.fit_transform(X))\n",
    "    \n",
    "    nmf_accs, pca_accs = [], []\n",
    "    for _ in range(pred_iterations):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_nmftrans, y)\n",
    "        logreg_model.fit(X_train, y_train)\n",
    "        y_pred = logreg_model.predict(X_test)\n",
    "        nmf_accs.append(accuracy_score(y_pred, y_test))\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_pcatrans, y)\n",
    "        logreg_model.fit(X_train, y_train)\n",
    "        y_pred = logreg_model.predict(X_test)\n",
    "        pca_accs.append(accuracy_score(y_pred, y_test))\n",
    "\n",
    "    dim_nmf_accs.append(np.mean(nmf_accs))\n",
    "    dim_pca_accs.append(np.mean(pca_accs))\n",
    "    \n",
    "nmf_logreg_accs = pd.Series(dim_nmf_accs)\n",
    "pca_logreg_accs = pd.Series(dim_pca_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = range(min_dim, min(X.shape) + 1, 10)\n",
    "nmf_logreg_accs.index = index\n",
    "pca_logreg_accs.index = index\n",
    "\n",
    "nmf_logreg_accs.plot.line(title='LogReg Accuracy for PCA and NMF Dim-Reduced Data', legend=True, label='NMF', figsize=(15,8))\n",
    "ax = pca_logreg_accs.plot.line(legend=True, label='PCA')\n",
    "ax.set_xlabel(\"Dimensions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
